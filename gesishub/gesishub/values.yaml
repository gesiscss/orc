etcJupyter:
  jupyter_notebook_config.json:
    NotebookApp:
      allow_origin: '*'
      tornado_settings:
        trust_xheaders: true
      # shutdown the server after no activity
      shutdown_no_activity_timeout: 1800

    # if a user leaves a notebook with a running kernel,
    # the effective idle timeout will typically be CULL_TIMEOUT + CULL_KERNEL_TIMEOUT
    # as culling the kernel will register activity,
    # resetting the no_activity timer for the server as a whole
    # Check also jupyterhub.cull.timeout config
    MappingKernelManager:
      # shutdown kernels after no activity
      cull_idle_timeout: 1800
      # check for idle kernels this often
      cull_interval: 60
      # a kernel with open connections but no activity still counts as idle
      # this is what allows us to shutdown servers
      # when people leave a notebook open and wander off
      cull_connected: true

persistent_binderhub:
  binderhub:
    config:
      GitHubRepoProvider:
        # Add banned repositories to the list below
        # They should be strings that will match "^<org-name>/<repo-name>.*"
        banned_specs:
          # e.g. '^org/repo.*'
          - ^ines/spacy-binder.*
          - ^soft4voip/rak.*
          - ^hmharshit/cn-ait.*
          - ^shishirchoudharygic/mltraining.*
          - ^hmharshit/mltraining.*
          - ^FDesnoyer/MathExp.*
          - ^GuitarsAI/.*
        high_quota_specs:
          - ^gesiscss/.*
        #spec_config:
        #  - pattern: ^gesiscss/.*
        #    config:
        #      quota: 200
      GitRepoProvider:
        banned_specs:
          - ^https%3A%2F%2Fbitbucket.org%2Fnikiubel%2Fnikiubel.bitbucket.io.git/.*
          - ^https%3A%2F%2Fjovian.ml%2Fapi%2Fgit%2F.*
          - ^(git|https?)%3A%2F%2Fnotabug.org%2FulslcuRux3Y%2F.*
          - ^https%3A%2F%2Fframagit.org%2FCecGhesq%2Flic_mdf_nsi_1.*
          - ^(git|https?)%3A%2F%2Fcodeberg.org%2Fforgiagether%2F.*

      BinderHub:
        build_image: quay.io/jupyterhub/repo2docker:2021.08.0-86.gbf99b93
        build_node_selector:
          user: worker
        per_repo_quota: 100
        per_repo_quota_higher: 200
        build_memory_limit: "8G"
        build_memory_request: "1G"

        template_path: /etc/binderhub/templates

    podAnnotations:
      rollme: "rollme"
    extraVolumes:
      - name: binder-templates
        configMap:
          name: binder-templates
      - name: binder-templates-gesis
        configMap:
          name: hub-templates-gesis
      - name: binder-extra-config-json
        configMap:
          name: hub-extra-config-json
    extraVolumeMounts:
      - name: binder-templates
        mountPath: /etc/binderhub/templates
      - name: binder-templates-gesis
        mountPath: /etc/binderhub/templates/gesis
      - name: binder-extra-config-json
        mountPath: /usr/local/etc/binderhub/extra_config.json
        subPath: extra_config.json

    extraConfig:
      01-template-variables:  |
        import json
        with open('/usr/local/etc/binderhub/extra_config.json') as extra_config_file:
            template_vars = json.load(extra_config_file)["template_vars"]
        import uuid
        template_vars.update({'user': None,
                              'logout_url': '/hub/logout',
                              #'help_url': 'https://www.gesis.org/en/help/',
                              "static_version": uuid.uuid4().hex,
                              "production": production,
                              })
        c.BinderHub.template_variables.update(template_vars)

    service:
      type: NodePort

    cors: &cors
      allowOrigin: '*'

    dind:
      enabled: false

    imageCleaner:
      enabled: false

    jupyterhub:
      custom:
        cors: *cors
      cull:
        # kill user pods if idle for 30 mins - default is 1 hour
        every: 660
        timeout: 1800
        # maxAge is 6 hours: 6 * 3600 = 21600
        maxAge: 21600
      hub:
        networkPolicy:
          # z2jh chart has a default ingress rule which allows inbound traffic
          # only to port 8081 (API port)
          # from pods with label "hub.jupyter.org/network-access-hub",
          # user and proxy pods have this label
          # z2jh chart has a default egress rule: allow all outbound traffic for hub
          enabled: true
        nodeSelector:
          base: worker  # where database is
        db:
          type: postgres

        # https://zero-to-jupyterhub.readthedocs.io/en/latest/administrator/authentication.html#genericoauthenticator-openid-connect
        config:
          Authenticator:
            admin_users: ['arnim.bleier@gesis.org', 'mridul.seth@gesis.org']
            enable_auth_state: true
          GenericOAuthenticator:
            auto_login: true
            userdata_method: GET
            userdata_params:
              state: state
            username_key: preferred_username
          JupyterHub:
            authenticator_class: oauthenticator.generic.GenericOAuthenticator

        annotations:
          # auto restart hub pod if any of the configmap files changes
          #checksum/config-map: {{ include (print $.Template.BasePath "/hub-configmap.yaml") . | sha256sum }}
          # the config above doesnt work, because templating is not allowed in values.yaml,,
          # so we have a default string "rollme", and in CI we calculate sha256sum of files folder
          # and update this config with helm upgrade ... --set persistent_binderhub.binderhub.jupyterhub.hub.annotations.rollme=<sha256sum>
          # https://helm.sh/docs/intro/using_helm/
          rollme: "rollme"
        extraVolumes:
          # the first 3 are taken from persistent_binderhub chart, because extraVolumes is a list
          - name: persistent-bhub-templates
            configMap:
              name: persistent-bhub-templates
          - name: persistent-bhub-statics
            configMap:
              name: persistent-bhub-statics
          - name: persistent-bhub-config
            configMap:
              name: persistent-bhub-config
          - name: hub-templates
            configMap:
              name: hub-templates
          - name: hub-templates-gesis
            configMap:
              name: hub-templates-gesis
          - name: hub-extra-config
            configMap:
              name: hub-extra-config
          - name: hub-extra-config-json
            configMap:
              name: hub-extra-config-json
          - name: hub-extra-users
            configMap:
              name: hub-extra-users
        extraVolumeMounts:
          # the first 3 are taken from persistent_binderhub chart, because extraVolumes is a list
          - name: persistent-bhub-templates
            mountPath: /etc/jupyterhub/templates
          - name: persistent-bhub-statics
            # mount to where jupyterhub static files are
            mountPath: /usr/local/share/jupyterhub/static/persistent_bhub
          - name: persistent-bhub-config
            # mount to where jupyterhub_config.py is,
            # we import the spawner class from persistent_bhub_config inside jupyterhub_config.py (hub.extraConfig)
            mountPath: /usr/local/etc/jupyterhub/persistent_bhub_config.py
            subPath: persistent_bhub_config.py
          - name: hub-templates
            mountPath: /etc/jupyterhub/orc_templates
          - name: hub-templates-gesis
            mountPath: /etc/jupyterhub/orc_templates/gesis
          - mountPath: /usr/local/etc/jupyterhub/extra_config.py  # mount where jupyterhub_config.py is, we import it there.
            subPath: extra_config.py
            name: hub-extra-config
          - mountPath: /usr/local/etc/jupyterhub/extra_config.json  # mount where jupyterhub_config.py is, we import it there.
            subPath: extra_config.json
            name: hub-extra-config-json
          - mountPath: /usr/local/etc/jupyterhub/_secret_user_id.json
            subPath: _secret_user_id.json
            name: hub-extra-users

        templatePaths:
          - "/etc/jupyterhub/orc_templates"
          - "/etc/jupyterhub/templates"  # from persistent_binderhub chart
        templateVars:
          persistency_explanation: "Have a Binder-Ready repository? With GESIS Notebooks, turn this repository into a persistent Jupyter environment, allowing you to continue your analysis from anywhere at any time."
        authenticatePrometheus: false
        shutdownOnLogout: true
        extraConfig:
          10-project-api: |
            from persistent_bhub_config import ProjectAPIHandler
            from extra_config import OrcAdminHandler, ORC_LOGIN_COOKIE_NAME, TakeoutData

            from jupyterhub.handlers import Template404
            c.JupyterHub.extra_handlers = [(r'/admin_orc', OrcAdminHandler),
                                            (r'/takeout', TakeoutData),
                                          (r'/api/projects/([^/]+)', ProjectAPIHandler),
                                          # return 404 for /hub/(about|faq|terms_of_use)/
                                          # /(about|faq|terms_of_use)/ pages are served in custom PrefixRedirectHandler
                                          (r'/about/', Template404),
                                          (r'/faq/', Template404),
                                          (r'/terms_of_use/', Template404),
                                          ]

            from jupyterhub.handlers.base import PrefixRedirectHandler
            from jupyterhub.utils import url_path_join
            import os
            def get(self):
                path = self.request.path
                if path == "/about/":
                    html = self.render_template("about.html", sync=True, **{"active": "about"})
                    self.finish(html)
                elif path == "/terms_of_use/":
                    html = self.render_template("terms_of_use.html", sync=True, **{"active": "terms_of_use"})
                    self.finish(html)
                elif path == "/faq/":
                    html = self.render_template("faq.html", sync=True, **{"active": "faq"})
                    self.finish(html)
                elif path == "/":
                    if self.get_cookie(ORC_LOGIN_COOKIE_NAME):
                        self.redirect("/hub/home")
                    else:
                        # user is not logged in, so link featured projects to (public) GESIS binder
                        binder_base_url = "/binder/"
                        html = self.render_template("orc_home.html", sync=True, **{"active": "home", "binder_base_url": binder_base_url})
                        self.finish(html)
                else:
                    # below is taken from original get method
                    uri = self.request.uri
                    # Since self.base_url will end with trailing slash.
                    # Ensure uri will end with trailing slash when matching
                    # with self.base_url.
                    if not uri.endswith('/'):
                        uri += '/'
                    if uri.startswith(self.base_url):
                        path = self.request.uri[len(self.base_url):]
                    else:
                        path = self.request.path
                    if not path:
                        # default / -> /hub/ redirect
                        # avoiding extra hop through /hub
                        path = '/'
                    self.redirect(url_path_join(self.hub.base_url, path), permanent=False)
            PrefixRedirectHandler.get = get
          021-orc: |
            from extra_config import template_vars
            template_vars.update({"production": production})
            c.JupyterHub.template_vars.update(template_vars)
          03-orc: |
            from extra_config import KeycloakLogoutHandler, KeycloakLoginHandler, KeycloakOAuthCallbackHandler
            from oauthenticator.generic import GenericOAuthenticator
            GenericOAuthenticator.default_logout_handler = KeycloakLogoutHandler
            GenericOAuthenticator.default_login_handler = KeycloakLoginHandler
            GenericOAuthenticator.callback_handler = KeycloakOAuthCallbackHandler
            def get_handlers(self, app):
                return [
                    (r'/oauth_login', self.login_handler),
                    (r'/oauth_callback', self.callback_handler),
                ] + [(r'/logout', self.default_logout_handler), (r'/login', self.default_login_handler)]
            GenericOAuthenticator.get_handlers = get_handlers
      proxy:
        https:
          enabled: true
          type: offload
        chp:
          networkPolicy:
            # z2jh chart has a default ingress rule which allows inbound traffic
            # to port 8000 (HTTP port) from pods with label "hub.jupyter.org/network-access-proxy-HTTP" and
            # to port 8001 (API port) from pods with label "hub.jupyter.org/network-access-proxy-API",
            # and only hub pod has these labels
            # so only the hub pod can talk to the proxy's API and HTTP ports
            # z2jh chart has a default egress rule: allow all outbound traffic for proxy
            enabled: true
        service:
          type: NodePort

      singleuser:
        networkPolicy:
          enabled: true
          # z2jh chart has a default ingress rule which allows inbound traffic
          # only to port 8888
          # from pods with label "hub.jupyter.org/network-access-singleuser",
          # hub and proxy pods have this label
          #ingress: []
          # z2jh chart has a default egress rule which restricts outbound traffic to only JupyterHub API port
          egress: []  # no additional egress rule, this empty list also overrides the egress rule defined in values.yaml of z2jh
        nodeSelector:
          user: worker
        storage:
          extraVolumes:
            - name: etc-jupyter
              configMap:
                name: user-etc-jupyter
          extraVolumeMounts:
            - name: etc-jupyter
              mountPath: /etc/jupyter
        # increase the start timeout of user server to 10 mins, because some repos creates big images, which makes is slow to pull
        startTimeout: 600  # default is 300 in z2jh chart

      # we dont need this because we have only 1 node for user pods
      scheduling:
        userScheduler:
          enabled: false
        podPriority:
          enabled: false
        userPlaceholder:
          enabled: false

