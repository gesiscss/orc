etcJupyter:
  jupyter_notebook_config.json:
    NotebookApp:
      allow_origin: '*'
      tornado_settings:
        trust_xheaders: true
      # shutdown the server after no activity
      shutdown_no_activity_timeout: 600

    # if a user leaves a notebook with a running kernel,
    # the effective idle timeout will typically be CULL_TIMEOUT + CULL_KERNEL_TIMEOUT
    # as culling the kernel will register activity,
    # resetting the no_activity timer for the server as a whole
    # Check also jupyterhub.cull.timeout config
    MappingKernelManager:
      # shutdown kernels after no activity
      cull_idle_timeout: 600
      # check for idle kernels this often
      cull_interval: 60
      # a kernel with open connections but no activity still counts as idle
      # this is what allows us to shutdown servers
      # when people leave a notebook open and wander off
      cull_connected: true

# this is set in _secret.yaml
# this is required to send GESIS Binder events to mybinder.org's StackDriver
eventsArchiver:
  serviceAccountKey: ""

binderhub:
  pdb:
    minAvailable: 1
  replicas: 2
  config:
    GitHubRepoProvider:
      # Add banned repositories to the list below
      # They should be strings that will match "^<org-name>/<repo-name>.*"
      banned_specs:
        # e.g. '^org/repo.*'
        - ^ines/spacy-binder.*
        - ^soft4voip/rak.*
        - ^hmharshit/cn-ait.*
        - ^shishirchoudharygic/mltraining.*
        - ^hmharshit/mltraining.*
      high_quota_specs:
        - ^gesiscss/.*
      #spec_config:
      #  - pattern: ^gesiscss/.*
      #    config:
      #      quota: 200
    BinderHub:
      pod_quota: 0
      base_url: /binder/
      use_registry: true
      build_image: jupyter/repo2docker:0.11.0-141.gf0c6f65
      build_node_selector:
        user: worker
      per_repo_quota: 100
      per_repo_quota_higher: 200

      template_path: /etc/binderhub/templates
      # look at configmap for static files here https://discourse.jupyter.org/t/customizing-jupyterhub-on-kubernetes/1769/4?u=bitniks
      #extra_static_path: /etc/binderhub/custom/gesisbinder/static
      #extra_static_url_prefix: /extra_static/

  annotations:
    rollme: "rollme"
  extraVolumes:
    - name: binder-templates
      configMap:
        name: binder-templates
    - name: binder-templates-gesis
      configMap:
        name: binder-templates-gesis
  extraVolumeMounts:
    - name: binder-templates
      mountPath: /etc/binderhub/templates
    - name: binder-templates-gesis
      mountPath: /etc/binderhub/templates/gesis

  extraConfig:
    01-template-variables:  |
      import uuid
      template_vars = {
          "version": "beta",
          "home_url": "/",
          "gesishub_url": "/hub/",
          "gesisbinder_url": "/binder/",
          "about_url": "/about/",
          "tou_url": "/terms_of_use/",
          "imprint_url": "https://www.gesis.org/en/institute/imprint/",
          "data_protection_url": "https://www.gesis.org/en/institute/data-protection/",
          "gesis_url": "https://www.gesis.org/en/home/",
          "gallery_url": "/gallery/",
          "faq_url": "/faq/",

          "active": "binder",
          "static_nginx": "/static/",

          "static_version": uuid.uuid4().hex,
          "user": None,

          "production": production,
      }
      c.BinderHub.template_variables.update(template_vars)
    11-eventlog: |
      from datetime import datetime
      import jsonschema
      import requests
      from tornado.log import app_log
      from time import sleep
      def emit(self, schema_name, version, event):
          """
          Emit event with given schema / version in a capsule.
          """
          if not self.handlers_maker:
              # If we don't have a handler setup, ignore everything
              return

          if (schema_name, version) not in self.schemas:
              raise ValueError(f'Schema {schema_name} version {version} not registered')
          schema = self.schemas[(schema_name, version)]
          jsonschema.validate(event, schema)

          capsule = {
              'timestamp': datetime.utcnow().isoformat() + 'Z',
              'schema': schema_name,
              'version': version
          }
          capsule.update(event)
          self.log.info(capsule)

          # token of binder user in gallery
          headers = {'Authorization': f'Bearer {os.environ["GESIS_API_TOKEN"]}'}
          api_url = os.environ["GESIS_API_URL"]

          # emit function is called after .launch, so we can wait before retry
          # this delay shouldn't effect the user
          retries = 3
          delay = 4
          try:
              for i in range(retries):
                  r = requests.post(api_url, data=capsule, headers=headers)
                  if r.status_code != 201 and i < 2:
                      sleep(delay)
                      delay *= 2
                  else:
                      break
              if r.status_code != 201:
                  app_log.error(f"Error: Event stream failed after {retries} retries for {capsule} with status code {r.status_code}")
          except Exception as e:
              app_log.error(f"Error: Event stream failed for {capsule}: {e}")

      from binderhub.events import EventLog
      EventLog.emit = emit

  service:
    type: NodePort

  cors: &cors
    allowOrigin: '*'

  dind:
    enabled: false

  imageCleaner:
    enabled: false

  jupyterhub:
    custom:
      cors: *cors
    cull:
      # cull every 11 minutes so it is out of phase
      # with the proxy check-routes interval of five minutes
      every: 660
      timeout: 600
      # maxAge is 6 hours: 6 * 3600 = 21600
      maxAge: 21600
    hub:
      # NOTE: hub and proxy must have 1 pod (https://github.com/jupyterhub/jupyterhub/issues/2841#issuecomment-561848594)
      # replicas: 1
      pdb:
        minAvailable: 0
      networkPolicy:
        # z2jh chart has a default ingress rule which allows inbound traffic
        # only to port 8081 (API port)
        # from pods with label "hub.jupyter.org/network-access-hub",
        # user and proxy pods have this label
        # z2jh chart has a default egress rule: allow all outbound traffic for hub
        enabled: true
      nodeSelector:
        base: worker  # where database is
      baseUrl: /binder/jupyter/
      db:
        type: postgres
      authenticatePrometheus: false
      extraConfig:
        02-orc: |
          c.KubeSpawner.extra_pod_config.update({'restart_policy': 'Never'})
    proxy:
      # NOTE: hub and proxy must have 1 pod (https://github.com/jupyterhub/jupyterhub/issues/2841#issuecomment-561848594)
      # replicas: 1
      pdb:
        minAvailable: 0
      https:
        type: offload
      networkPolicy:
        # z2jh chart has a default ingress rule which allows inbound traffic
        # to port 8000 (HTTP port) from pods with label "hub.jupyter.org/network-access-proxy-HTTP" and
        # to port 8001 (API port) from pods with label "hub.jupyter.org/network-access-proxy-API",
        # and only hub pod has these labels
        # so only the hub pod can talk to the proxy's API and HTTP ports
        # z2jh chart has a default egress rule: allow all outbound traffic for proxy
        enabled: true
      service:
        type: NodePort
    singleuser:
      networkPolicy:
        enabled: true
        # z2jh chart has a default ingress rule which allows inbound traffic
        # only to port 8888
        # from pods with label "hub.jupyter.org/network-access-singleuser",
        # hub and proxy pods have this label
        #ingress: []
        # z2jh chart has a default egress rule which restricts outbound traffic to only JupyterHub API port
        #egress: []  # no additional egress rule, this empty list also overrides the egress rule defined in values.yaml of z2jh
        egress:
          # These egress rules are copied from mybinder.org-deploy repo
          # (https://github.com/jupyterhub/mybinder.org-deploy/blob/master/mybinder/templates/netpol.yaml)
          # We also allow outbound traffic to mongodb and mysql
          # allow DNS resolution in the cluster
          # it would be nicer to be able to pin this to only the kube-dns service,
          # but we can only seem to select *pods* not *services* as the destination
          - ports:
              - port: 53
                protocol: TCP
              - port: 53
                protocol: UDP
            to:
              - ipBlock:
                  cidr: 10.0.0.0/8  # we need this range for kubernetes services
              # we changed the Pod network CIDR to 10.244.0.0/16 for calico
              #- ipBlock:
              #    # TODO do we really need this for allowing DNS in cluster
              #    cidr: 192.168.0.0/16  # Calico by default uses 192.168.0.0/16 as the Pod network CIDR
          # allow access to the world,
          # but not the cluster
          # https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network
          # By default, Calico uses 192.168.0.0/16 as the Pod network CIDR
          # For flannel to work correctly, you must pass --pod-network-cidr=10.244.0.0/16
          # For Cilium to work correctly, you must pass --pod-network-cidr=10.217.0.0/16 to kubeadm init.
          # 10.244.0.0/16 and 10.217.0.0/16 must be covered by 10.0.0.0/8
          - ports:
              # TCP is the default protocol
              - port: 80
              - port: 443
              - port: 9418 # git
              - port: 873 # rsync
              - port: 1094 # xroot
              - port: 1095 # xroot
              - port: 27017 # mongodb
              - port: 3306  # mysql
            to:
              - ipBlock:
                  cidr: 0.0.0.0/0  # represents all possible IP addresses
                  # we changed the Pod network CIDR to 10.244.0.0/16 for calico
                  except:
                    #- 192.168.0.0/16  # Calico uses 192.168.0.0/16 as the Pod network CIDR
                    - 169.254.169.254/32
                    - 10.0.0.0/8
              - ipBlock:
                  cidr: 10.6.13.139/32  # mongodb - https://github.com/gesiscss/btw17_sample_scripts
      nodeSelector:
        user: worker
      storage:
        extraVolumes:
          - name: etc-jupyter
            configMap:
              name: user-etc-jupyter
          - name: etc-jupyter-templates
            configMap:
              name: user-etc-jupyter-templates
        extraVolumeMounts:
          - name: etc-jupyter
            mountPath: /etc/jupyter
          - name: etc-jupyter-templates
            mountPath: /etc/jupyter/templates

    scheduling:
      userScheduler:
        enabled: false
      podPriority:
        enabled: false
      userPlaceholder:
        enabled: false

imageCleaner:
  enabled: false
  image:
    name: gesiscss/orc-image-cleaner
    tag: dc975e2
  # start deleting images which are not used in x days
  days: 7
  # interval (in seconds) between cleaning processes
  interval: 300
  # path to check available disk space
  path: /var/lib/docker
  # delete an image at most every 5 seconds
  delay: 5
  # start cleaning when available disk space is <= 10%
  # until it increases to/above 30%
  highLimit: 90
  lowLimit: 70
  # cull images on the host docker
  dockerSocket: /var/run/docker.sock
  # dockerLibDir: /var/lib/docker
  # we clean ~20% -> ~400GB on ssd disk
  dockerLibDir: /ssd/docker

